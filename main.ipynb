{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinyphysics import TinyPhysicsModel, TinyPhysicsSimulator, CONTROL_START_IDX\n",
    "from controllers import pid, BaseController, bayesianController\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from collections import namedtuple\n",
    "\n",
    "State = namedtuple('State', ['roll_lataccel', 'v_ego', 'a_ego'])\n",
    "\n",
    "class MyPhysicsModel(TinyPhysicsModel):\n",
    "    def __init__(self, model_path: str, debug: bool) -> None:\n",
    "        super().__init__(model_path, debug)\n",
    "        \n",
    "    def get_current_lataccel(self, sim_states: List[State], actions: List[float], past_preds: List[float]) -> float:\n",
    "        tokenized_actions = self.tokenizer.encode(past_preds)\n",
    "        raw_states = [list(x) for x in sim_states]\n",
    "        states = np.column_stack([actions, raw_states])\n",
    "        input_data = {\n",
    "          'states': np.expand_dims(states, axis=0).astype(np.float32),\n",
    "          'tokens': np.expand_dims(tokenized_actions, axis=0).astype(np.int64)\n",
    "        }\n",
    "        return self.tokenizer.decode(self.predict(input_data, temperature=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC_G = 9.81\n",
    "FPS = 10\n",
    "CONTROL_START_IDX = 100\n",
    "COST_END_IDX = 500\n",
    "CONTEXT_LENGTH = 20\n",
    "VOCAB_SIZE = 1024\n",
    "LATACCEL_RANGE = [-5, 5]\n",
    "STEER_RANGE = [-2, 2]\n",
    "MAX_ACC_DELTA = 0.5\n",
    "DEL_T = 0.1\n",
    "LAT_ACCEL_COST_MULTIPLIER = 50.0\n",
    "\n",
    "FUTURE_PLAN_STEPS = FPS * 5  # 5 secs\n",
    "from typing import List, Union, Tuple\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "\n",
    "FuturePlan = namedtuple('FuturePlan', ['lataccel', 'roll_lataccel', 'v_ego', 'a_ego'])\n",
    "\n",
    "class MySimulator(TinyPhysicsSimulator):\n",
    "    \n",
    "    def __init__(self, model: TinyPhysicsModel, data_path: str, controller: BaseController, debug: bool = False) -> None:\n",
    "        super().__init__(model, data_path, controller, debug)\n",
    "        \n",
    "        \n",
    "    def sim_step(self, step_idx: int, force_action=False) -> None:\n",
    "        try:\n",
    "            pred = self.sim_model.get_current_lataccel(\n",
    "              sim_states=self.state_history[-CONTEXT_LENGTH:],\n",
    "              actions=self.action_history[-CONTEXT_LENGTH:],\n",
    "              past_preds=self.current_lataccel_history[-CONTEXT_LENGTH:]\n",
    "            )\n",
    "        except:\n",
    "            print(self.state_history[-CONTEXT_LENGTH:], self.action_history[-CONTEXT_LENGTH:], self.current_lataccel_history[-CONTEXT_LENGTH:])\n",
    "        pred = np.clip(pred, self.current_lataccel - MAX_ACC_DELTA, self.current_lataccel + MAX_ACC_DELTA)\n",
    "        if step_idx >= CONTROL_START_IDX or force_action:\n",
    "            self.current_lataccel = pred\n",
    "        else:\n",
    "            self.current_lataccel = self.get_state_target_futureplan(step_idx)[1]\n",
    "        self.current_lataccel_history.append(self.current_lataccel)\n",
    "\n",
    "        \n",
    "    def control_step(self, step_idx: int, steer_action: float, force_action=False) -> None:\n",
    "        #action = self.controller.update(self.target_lataccel_history[step_idx], self.current_lataccel, self.state_history[step_idx], future_plan=self.futureplan)\n",
    "        if step_idx >= CONTROL_START_IDX and steer_action is not None:\n",
    "            action = steer_action\n",
    "        else:\n",
    "            action = self.data['steer_command'].values[step_idx]\n",
    "        action = np.clip(action, STEER_RANGE[0], STEER_RANGE[1])\n",
    "        self.action_history.append(action)\n",
    "        \n",
    "    def get_state_target_futureplan(self, step_idx: int) -> Tuple[State, float]:\n",
    "        state = self.data.iloc[step_idx]\n",
    "        return (\n",
    "            State(roll_lataccel=state['roll_lataccel'], v_ego=state['v_ego'], a_ego=state['a_ego']),\n",
    "            state['target_lataccel'],\n",
    "            FuturePlan(\n",
    "                lataccel=self.data['target_lataccel'].values[step_idx + 1 :step_idx + FUTURE_PLAN_STEPS].tolist(),\n",
    "                roll_lataccel=self.data['roll_lataccel'].values[step_idx + 1 :step_idx + FUTURE_PLAN_STEPS].tolist(),\n",
    "                v_ego=self.data['v_ego'].values[step_idx + 1 :step_idx + FUTURE_PLAN_STEPS].tolist(),\n",
    "                a_ego=self.data['a_ego'].values[step_idx + 1 :step_idx + FUTURE_PLAN_STEPS].tolist()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def step(self, steer_action=None, force_action=False) -> None:\n",
    "        state, target, futureplan = self.get_state_target_futureplan(self.step_idx)\n",
    "        self.state_history.append(state)\n",
    "        self.target_lataccel_history.append(target)\n",
    "        self.futureplan = futureplan\n",
    "        self.control_step(self.step_idx, steer_action, force_action)\n",
    "        self.sim_step(self.step_idx, force_action)\n",
    "        self.step_idx += 1\n",
    "        \n",
    "    def reverse_step(self) -> None:\n",
    "        self.step_idx -= 1\n",
    "        self.state_history.pop()\n",
    "        self.target_lataccel_history.pop()\n",
    "        self.action_history.pop()\n",
    "        self.current_lataccel_history.pop()\n",
    "        state, target, futureplan = self.get_state_target_futureplan(self.step_idx)\n",
    "        self.futureplan = futureplan\n",
    "        self.current_lataccel = self.current_lataccel_history[-1]\n",
    "        \n",
    "    def compute_cost(self, unit=False) -> dict:\n",
    "        if unit:\n",
    "            target = np.array(self.target_lataccel_history)[self.step_idx - 2:self.step_idx]\n",
    "            pred = np.array(self.current_lataccel_history)[self.step_idx - 2:self.step_idx]\n",
    "        else:\n",
    "            target = np.array(self.target_lataccel_history)[CONTROL_START_IDX:COST_END_IDX]\n",
    "            pred = np.array(self.current_lataccel_history)[CONTROL_START_IDX:COST_END_IDX]\n",
    "            \n",
    "        lat_accel_cost = np.mean((target - pred)**2) * 100\n",
    "        jerk_cost = np.mean((np.diff(pred) / DEL_T)**2) * 100\n",
    "            \n",
    "        total_cost = (lat_accel_cost * LAT_ACCEL_COST_MULTIPLIER) + jerk_cost\n",
    "        return {'lataccel_cost': lat_accel_cost, 'jerk_cost': jerk_cost, 'total_cost': total_cost}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineRecord:\n",
    "    \n",
    "    def __init__(self, baseline):\n",
    "        self.current_lataccel_history = baseline.current_lataccel_history\n",
    "        self.target_lataccel_history = baseline.target_lataccel_history\n",
    "        self.state_history = baseline.state_history\n",
    "        self.action_history = baseline.action_history\n",
    "        self.cost = baseline.compute_cost()\n",
    "    \n",
    "    def compute_cost(self):\n",
    "        return self.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('baselines.pkl', 'rb') as handle:\n",
    "    baselines = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rollout(sim):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(sim.target_lataccel_history, label=\"Target Lateral Acceleration\", alpha=0.5)\n",
    "    ax.plot(sim.current_lataccel_history, label=\"Actual Lateral Acceleration\", alpha=0.5)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Step\")\n",
    "    ax.set_ylabel(\"Lateral Acceleration\")\n",
    "    ax.set_title(\"Rollout\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driving_model = TinyPhysicsModel(\"./models/tinyphysics.onnx\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driving_model = MyPhysicsModel(\"./models/tinyphysics.onnx\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = TinyPhysicsModel(\"./models/tinyphysics.onnx\", debug=True)\n",
    "controller = pid.Controller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testController2 = Controller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testController3 = bayesianController.Controller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def execute_test(filename, controller, driving_model, plot=False):\n",
    "    sim = TinyPhysicsSimulator(driving_model, filename, controller=controller, debug=False)\n",
    "    cost = sim.rollout()\n",
    "    if plot:\n",
    "        plot_rollout(sim)\n",
    "    return cost\n",
    "    \n",
    "def compare_with_baseline(driving_model, filename, baseline_controller, test_controller):\n",
    "    baseline_cost = execute_test(filename, baseline_controller, driving_model, plot=False)\n",
    "    cost = execute_test(filename, test_controller, driving_model, plot=False)\n",
    "    return baseline_cost, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant files: 03067, 02675, 19960, 02208, 15947, 01037, 01581, 02894, 00522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noisy_driving_model = TinyPhysicsModel(\"./models/tinyphysics.onnx\", debug=True)\n",
    "sim = TinyPhysicsSimulator(noisy_driving_model, \"./data/00000.csv\", controller=testController3, debug=False)\n",
    "sim.rollout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.compute_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testController2 = Controller()\n",
    "#testController2 = pid.Controller()\n",
    "noisy_driving_model = TinyPhysicsModel(\"./models/tinyphysics.onnx\", debug=True)\n",
    "sim = TinyPhysicsSimulator(noisy_driving_model, \"./data/01037.csv\", controller=testController2, debug=False)\n",
    "#for i in range(20, 150):\n",
    "#    sim.step()\n",
    "sim.rollout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.compute_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rollout(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rollout(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from controllers import BaseController\n",
    "import numpy as np\n",
    "class Controller(BaseController):\n",
    "  \"\"\"\n",
    "  A simple PID controller\n",
    "  \"\"\"\n",
    "  def __init__(self,):\n",
    "    self.p = 0.085\n",
    "    self.i = 0.11\n",
    "    self.d = -0.0\n",
    "    self.error_integral = 0\n",
    "    self.prev_error = 0\n",
    "    self.n_updates= 0\n",
    "\n",
    "  def update(self, target_lataccel, current_lataccel, state, future_plan):\n",
    "      error = target_lataccel - current_lataccel\n",
    "      self.error_integral += error\n",
    "      error_diff = error - self.prev_error\n",
    "      self.prev_error = error\n",
    "      p = self.p\n",
    "      i = self.i\n",
    "      d = self.d\n",
    "      self.n_updates += 1\n",
    "      return p * error + i * self.error_integral + d * error_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from controllers import BaseController\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "import pickle\n",
    "from collections import namedtuple\n",
    "from bayes_opt import UtilityFunction\n",
    "\n",
    "PIDPolicy = namedtuple('PIDPolicy', ['p', 'i', 'd'])\n",
    "MAX_ACC_DELTA = 0.5\n",
    "\n",
    "class BayesianPlanningController2(BaseController):\n",
    "  \"\"\"\n",
    "  A simple PID controller\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    self.driving_model = MyPhysicsModel(\"./models/tinyphysics.onnx\", debug=True) #MyPhysicsModel\n",
    "    self.current_policy = PIDPolicy(p=0.085, i=0.11, d=0)\n",
    "    self.error_integral = 0\n",
    "    self.prev_error = 0\n",
    "    self.action_history = []\n",
    "    self.state_history = []\n",
    "    self.preds_history = []\n",
    "    self.costs = []\n",
    "    self.prev_target = -np.inf\n",
    "    self.change_policy_in = 0\n",
    "    self.n_updates = 0\n",
    "    \n",
    "  def compute_action(self, target_lataccel, current_lataccel, p=None, i=None, d=None):\n",
    "    error = target_lataccel - current_lataccel\n",
    "    self.error_integral += error\n",
    "    error_diff = error - self.prev_error\n",
    "    self.prev_error = error\n",
    "    p = self.current_policy.p if p is None else p\n",
    "    i = self.current_policy.i if i is None else i\n",
    "    d = self.current_policy.d if d is None else d\n",
    "    act = np.clip(p * error + i * self.error_integral + d * error_diff, STEER_RANGE[0], STEER_RANGE[1])\n",
    "    return act\n",
    "\n",
    "  def wrapper_evaluate_path(self, model, state_dict):\n",
    "    def evaluate_path(p, i, d):\n",
    "        error_integral = state_dict['ERROR_INTEGRAL']\n",
    "        prev_error = state_dict['PREV_ERROR']\n",
    "        actions = state_dict['ACTIONS'].copy()\n",
    "        sim_states = state_dict['SIM_STATES'].copy()\n",
    "        past_preds = state_dict['PAST_PREDS'].copy()\n",
    "        future = state_dict['FUTURE_PLAN']\n",
    "        current_lataccel = past_preds[-1]\n",
    "        plan_steps = len(future.lataccel)\n",
    "        for idx in range(plan_steps):\n",
    "            error = (future.lataccel[idx] - current_lataccel)\n",
    "            error_integral += error\n",
    "            error_diff = error - prev_error\n",
    "            prev_error = error\n",
    "            action = np.clip(p * error + i * error_integral + d * error_diff, STEER_RANGE[0], STEER_RANGE[1])\n",
    "            actions.append(action)\n",
    "            pred = model.get_current_lataccel(\n",
    "              sim_states=sim_states[-CONTEXT_LENGTH:],\n",
    "              actions=actions[-CONTEXT_LENGTH:],\n",
    "              past_preds=past_preds[-CONTEXT_LENGTH:]\n",
    "            )\n",
    "            pred = np.clip(pred, current_lataccel - MAX_ACC_DELTA, current_lataccel + MAX_ACC_DELTA)\n",
    "            past_preds.append(pred)\n",
    "            if idx < plan_steps - 1:\n",
    "                sim_states.append(State(a_ego=future.a_ego[idx], v_ego=future.v_ego[idx], roll_lataccel=future.roll_lataccel[idx]))\n",
    "            current_lataccel = pred\n",
    "        cost = compute_cost(np.array(future.lataccel[:plan_steps]), np.array(past_preds[-plan_steps:]))\n",
    "        self.costs.append(cost)\n",
    "        if self.different_policy:\n",
    "            current_policy_jerk = self.costs[1]['jerk_cost']\n",
    "            min_jerk = self.costs[0]['jerk_cost'] if self.costs[0]['jerk_cost'] < current_policy_jerk else current_policy_jerk\n",
    "        else:\n",
    "            min_jerk = self.costs[0]['jerk_cost']\n",
    "        if min_jerk < 50:\n",
    "            mul = 1 if cost['jerk_cost'] < min_jerk*2 else 100\n",
    "        else:\n",
    "            mul = 1 if cost['jerk_cost'] < min_jerk*1.3 else 100\n",
    "        #if self.costs[0]['lataccel_cost'] > cost['lataccel_cost'] and self.costs[0]['jerk_cost']*0.8 > cost['jerk_cost']:\n",
    "        #    mul = 0.01\n",
    "        #elif self.costs[0]['lataccel_cost'] > cost['lataccel_cost'] and self.costs[0]['jerk_cost'] < cost['jerk_cost']:\n",
    "        #    if self.costs[0]['jerk_cost'] < 50:\n",
    "        #        mul = 1 if cost['jerk_cost'] < self.costs[0]['jerk_cost']*2 else 100\n",
    "        #    else:\n",
    "        #        mul = 1 if cost['jerk_cost'] < self.costs[0]['jerk_cost']*1.3 else 100\n",
    "        #else:\n",
    "        #    mul = 1\n",
    "        #print(p, i, d, cost, 1/(cost['total_cost']*mul))\n",
    "        return 1/(cost['total_cost']*mul)\n",
    "    return evaluate_path\n",
    "\n",
    "  def update(self, target_lataccel, current_lataccel, state, future_plan):\n",
    "    self.n_updates += 1\n",
    "    self.state_history.append(state)\n",
    "    self.preds_history.append(current_lataccel)\n",
    "    self.change_policy_in -= 1\n",
    "    if len(self.action_history) >= 20 and self.change_policy_in <= 0 and len(future_plan.lataccel) >= 45:\n",
    "        self.costs = []\n",
    "        self.different_policy = False\n",
    "        state_dict = {\n",
    "            'PREV_ERROR': self.prev_error,\n",
    "            'ERROR_INTEGRAL': self.error_integral,\n",
    "            'ACTIONS': self.action_history,\n",
    "            'SIM_STATES': self.state_history,\n",
    "            'PAST_PREDS': self.preds_history,\n",
    "            'FUTURE_PLAN': FuturePlan(lataccel=[target_lataccel] + future_plan.lataccel, a_ego=future_plan.a_ego, v_ego=future_plan.v_ego, roll_lataccel=future_plan.roll_lataccel)\n",
    "        }\n",
    "        wrapper_function = self.wrapper_evaluate_path(self.driving_model, state_dict)\n",
    "\n",
    "        # Bounded region of parameter space\n",
    "        pbounds = {'p': (0.05, 0.6), 'i': (0.02, 0.2), 'd': (-0.3, 0.3)}\n",
    "\n",
    "        optimizer = BayesianOptimization(\n",
    "            f=wrapper_function,\n",
    "            pbounds=pbounds,\n",
    "            random_state=1,\n",
    "            allow_duplicate_points=True,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        optimizer.probe(\n",
    "            params={'p': 0.085, 'i': 0.11, 'd': -0.0},\n",
    "            lazy=False,\n",
    "        )\n",
    "        current_target = 1/optimizer.res[0]['target']\n",
    "        if current_target <= 90 and self.current_policy == PIDPolicy(p=0.085, i=0.11, d=0):\n",
    "            self.current_policy = PIDPolicy(p=0.085, i=0.11, d=0)\n",
    "            self.change_policy_in = 50\n",
    "        else:\n",
    "            if self.current_policy is not None:\n",
    "                if self.current_policy.p != 0.3 and self.current_policy.p != 0.085:\n",
    "                    #self.different_policy = True\n",
    "                    optimizer.probe(\n",
    "                        params={'p': self.current_policy.p, 'i': self.current_policy.i, 'd': self.current_policy.d},\n",
    "                        lazy=False,\n",
    "                    )\n",
    "                acquisition_function = UtilityFunction(kind=\"ei\", xi=1e-4)\n",
    "            else:\n",
    "                print(\"Finding initial guess\")\n",
    "                acquisition_function = UtilityFunction(kind=\"ucb\", kappa=10)\n",
    "                \n",
    "            optimizer.probe(\n",
    "                params={'p': 0.3, 'i': 0.05, 'd': -0.1},\n",
    "                lazy=True,\n",
    "            )\n",
    "            #optimizer.probe(\n",
    "            #    params={'p': 0.3, 'i': 0.0, 'd': -0.0},\n",
    "            #    lazy=True,\n",
    "            #)\n",
    "\n",
    "            optimizer.maximize(\n",
    "                init_points=10 if self.prev_target == -np.inf or current_target > 400 else 1, # 5\n",
    "                acquisition_function=acquisition_function,\n",
    "                n_iter=20 if self.prev_target == -np.inf else 5 # 25, 5 self.prev_target < 0.01\n",
    "            )\n",
    "            solution = optimizer.max['params']\n",
    "            if self.current_policy != PIDPolicy(p=0.085, i=0.11, d=0):\n",
    "                threshold = 0.85\n",
    "            else:\n",
    "                if current_target <= 90:\n",
    "                    threshold = 0\n",
    "                else:\n",
    "                    threshold = 0.7\n",
    "            if 1/optimizer.max['target'] < (current_target*threshold):\n",
    "                #print(f\"changing policy to p={solution['p']}, i={solution['i']}, d={solution['d']}\")\n",
    "                self.current_policy = PIDPolicy(p=solution['p'], i=solution['i'], d=solution['d'])\n",
    "                #self.current_policy = PIDPolicy(p=0.085, i=0.11, d=0)\n",
    "            else:\n",
    "                self.current_policy = PIDPolicy(p=0.085, i=0.11, d=0)\n",
    "            self.change_policy_in = 5 if optimizer.max['target'] > 0.005 else 10 # 50, 15\n",
    "        self.prev_target = optimizer.max['target']\n",
    "\n",
    "    action = self.compute_action(target_lataccel, current_lataccel)\n",
    "    self.action_history.append(action)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "DEL_T = 0.1\n",
    "LAT_ACCEL_COST_MULTIPLIER = 50\n",
    "\n",
    "def compute_cost(target, pred) -> dict:\n",
    "    lat_accel_cost = np.mean((target - pred)**2) * 100\n",
    "    jerk_cost = np.mean((np.diff(pred) / DEL_T)**2) * 100\n",
    "    total_cost = (lat_accel_cost * LAT_ACCEL_COST_MULTIPLIER) + jerk_cost\n",
    "\n",
    "    return {'lataccel_cost': lat_accel_cost, 'jerk_cost': jerk_cost, 'total_cost': total_cost}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
