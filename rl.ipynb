{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinyphysics import TinyPhysicsModel, TinyPhysicsSimulator, CONTROL_START_IDX\n",
    "from controllers import pid, BaseController\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from collections import namedtuple\n",
    "\n",
    "State = namedtuple('State', ['roll_lataccel', 'v_ego', 'a_ego'])\n",
    "\n",
    "class MyPhysicsModel(TinyPhysicsModel):\n",
    "    def __init__(self, model_path: str, debug: bool) -> None:\n",
    "        super().__init__(model_path, debug)\n",
    "        \n",
    "    def get_current_lataccel(self, sim_states: List[State], actions: List[float], past_preds: List[float]) -> float:\n",
    "        tokenized_actions = self.tokenizer.encode(past_preds)\n",
    "        raw_states = [list(x) for x in sim_states]\n",
    "        states = np.column_stack([actions, raw_states])\n",
    "        input_data = {\n",
    "          'states': np.expand_dims(states, axis=0).astype(np.float32),\n",
    "          'tokens': np.expand_dims(tokenized_actions, axis=0).astype(np.int64)\n",
    "        }\n",
    "        return self.tokenizer.decode(self.predict(input_data, temperature=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC_G = 9.81\n",
    "FPS = 10\n",
    "CONTROL_START_IDX = 100\n",
    "COST_END_IDX = 500\n",
    "CONTEXT_LENGTH = 20\n",
    "VOCAB_SIZE = 1024\n",
    "LATACCEL_RANGE = [-5, 5]\n",
    "STEER_RANGE = [-2, 2]\n",
    "MAX_ACC_DELTA = 0.5\n",
    "DEL_T = 0.1\n",
    "LAT_ACCEL_COST_MULTIPLIER = 50.0\n",
    "\n",
    "FUTURE_PLAN_STEPS = FPS * 5  # 5 secs\n",
    "from typing import List, Union, Tuple\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "\n",
    "FuturePlan = namedtuple('FuturePlan', ['lataccel', 'roll_lataccel', 'v_ego', 'a_ego'])\n",
    "\n",
    "class MySimulator(TinyPhysicsSimulator):\n",
    "    \n",
    "    def __init__(self, model: TinyPhysicsModel, data_path: str, controller: BaseController, debug: bool = False) -> None:\n",
    "        super().__init__(model, data_path, controller, debug)\n",
    "        \n",
    "        \n",
    "    def sim_step(self, step_idx: int, force_action=False) -> None:\n",
    "        try:\n",
    "            pred = self.sim_model.get_current_lataccel(\n",
    "              sim_states=self.state_history[-CONTEXT_LENGTH:],\n",
    "              actions=self.action_history[-CONTEXT_LENGTH:],\n",
    "              past_preds=self.current_lataccel_history[-CONTEXT_LENGTH:]\n",
    "            )\n",
    "        except:\n",
    "            print(self.state_history[-CONTEXT_LENGTH:], self.action_history[-CONTEXT_LENGTH:], self.current_lataccel_history[-CONTEXT_LENGTH:])\n",
    "        pred = np.clip(pred, self.current_lataccel - MAX_ACC_DELTA, self.current_lataccel + MAX_ACC_DELTA)\n",
    "        if step_idx >= CONTROL_START_IDX or force_action:\n",
    "            self.current_lataccel = pred\n",
    "        else:\n",
    "            self.current_lataccel = self.get_state_target_futureplan(step_idx)[1]\n",
    "        self.current_lataccel_history.append(self.current_lataccel)\n",
    "\n",
    "        \n",
    "    def control_step(self, step_idx: int, steer_action: float, force_action=False) -> None:\n",
    "        #action = self.controller.update(self.target_lataccel_history[step_idx], self.current_lataccel, self.state_history[step_idx], future_plan=self.futureplan)\n",
    "        #if force_action or (step_idx >= CONTROL_START_IDX and steer_action is not None and not pd.isna(steer_action)):\n",
    "        if step_idx >= CONTROL_START_IDX and steer_action is not None:\n",
    "            action = steer_action\n",
    "        else:\n",
    "            action = self.data['steer_command'].values[step_idx]\n",
    "        action = np.clip(action, STEER_RANGE[0], STEER_RANGE[1])\n",
    "        self.action_history.append(action)\n",
    "        \n",
    "    def get_state_target_futureplan(self, step_idx: int) -> Tuple[State, float]:\n",
    "        state = self.data.iloc[step_idx]\n",
    "        return (\n",
    "            State(roll_lataccel=state['roll_lataccel'], v_ego=state['v_ego'], a_ego=state['a_ego']),\n",
    "            state['target_lataccel'],\n",
    "            FuturePlan(\n",
    "                lataccel=self.data['target_lataccel'].values[step_idx + 1 :step_idx + FUTURE_PLAN_STEPS].tolist(),\n",
    "                roll_lataccel=self.data['roll_lataccel'].values[step_idx + 1 :step_idx + FUTURE_PLAN_STEPS].tolist(),\n",
    "                v_ego=self.data['v_ego'].values[step_idx + 1 :step_idx + FUTURE_PLAN_STEPS].tolist(),\n",
    "                a_ego=self.data['a_ego'].values[step_idx + 1 :step_idx + FUTURE_PLAN_STEPS].tolist()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def step(self, steer_action=None, force_action=False) -> None:\n",
    "        state, target, futureplan = self.get_state_target_futureplan(self.step_idx)\n",
    "        self.state_history.append(state)\n",
    "        self.target_lataccel_history.append(target)\n",
    "        self.futureplan = futureplan\n",
    "        self.control_step(self.step_idx, steer_action, force_action)\n",
    "        self.sim_step(self.step_idx, force_action)\n",
    "        self.step_idx += 1\n",
    "        \n",
    "    def reverse_step(self) -> None:\n",
    "        self.step_idx -= 1\n",
    "        self.state_history.pop()\n",
    "        self.target_lataccel_history.pop()\n",
    "        self.action_history.pop()\n",
    "        self.current_lataccel_history.pop()\n",
    "        state, target, futureplan = self.get_state_target_futureplan(self.step_idx)\n",
    "        self.futureplan = futureplan\n",
    "        self.current_lataccel = self.current_lataccel_history[-1]\n",
    "        \n",
    "    def compute_cost(self, unit=False) -> dict:\n",
    "        if unit:\n",
    "            target = np.array(self.target_lataccel_history)[self.step_idx - 2:self.step_idx]\n",
    "            pred = np.array(self.current_lataccel_history)[self.step_idx - 2:self.step_idx]\n",
    "        else:\n",
    "            target = np.array(self.target_lataccel_history)[CONTROL_START_IDX:COST_END_IDX]\n",
    "            pred = np.array(self.current_lataccel_history)[CONTROL_START_IDX:COST_END_IDX]\n",
    "            \n",
    "        lat_accel_cost = np.mean((target - pred)**2) * 100\n",
    "        jerk_cost = np.mean((np.diff(pred) / DEL_T)**2) * 100\n",
    "            \n",
    "        total_cost = (lat_accel_cost * LAT_ACCEL_COST_MULTIPLIER) + jerk_cost\n",
    "        return {'lataccel_cost': lat_accel_cost, 'jerk_cost': jerk_cost, 'total_cost': total_cost}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineRecord:\n",
    "    \n",
    "    def __init__(self, baseline):\n",
    "        self.current_lataccel_history = baseline.current_lataccel_history\n",
    "        self.target_lataccel_history = baseline.target_lataccel_history\n",
    "        self.state_history = baseline.state_history\n",
    "        self.action_history = baseline.action_history\n",
    "        self.cost = baseline.compute_cost()\n",
    "    \n",
    "    def compute_cost(self):\n",
    "        return self.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('baselines.pkl', 'rb') as handle:\n",
    "    baselines = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rollout(sim):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(sim.target_lataccel_history, label=\"Target Lateral Acceleration\", alpha=0.5)\n",
    "    ax.plot(sim.current_lataccel_history, label=\"Actual Lateral Acceleration\", alpha=0.5)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Step\")\n",
    "    ax.set_ylabel(\"Lateral Acceleration\")\n",
    "    ax.set_title(\"Rollout\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driving_model = TinyPhysicsModel(\"./models/tinyphysics.onnx\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driving_model = MyPhysicsModel(\"./models/tinyphysics.onnx\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = TinyPhysicsModel(\"./models/tinyphysics.onnx\", debug=True)\n",
    "controller = pid.Controller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testController2 = Controller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testController2 = Controller()\n",
    "noisy_driving_model = TinyPhysicsModel(\"./models/tinyphysics.onnx\", debug=True)\n",
    "sim = TinyPhysicsSimulator(noisy_driving_model, \"./data/02675.csv\", controller=testController2, debug=False)\n",
    "sim.rollout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.compute_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rollout(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from controllers import BaseController\n",
    "import numpy as np\n",
    "class Controller(BaseController):\n",
    "  \"\"\"\n",
    "  A simple PID controller\n",
    "  \"\"\"\n",
    "  def __init__(self,):\n",
    "    self.p = 0.085\n",
    "    self.i = 0.11\n",
    "    self.d = -0.0\n",
    "    self.error_integral = 0\n",
    "    self.prev_error = 0\n",
    "    self.n_updates= 0\n",
    "\n",
    "  def update(self, target_lataccel, current_lataccel, state, future_plan):\n",
    "      error = (target_lataccel - current_lataccel)\n",
    "      self.error_integral += error\n",
    "      error_diff = error - self.prev_error\n",
    "      self.prev_error = error\n",
    "      p = self.p\n",
    "      i = self.i\n",
    "      d = self.d\n",
    "      self.n_updates += 1\n",
    "      return p * error + i * self.error_integral + d * error_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "dfs = []\n",
    "to_load = 1\n",
    "for i in tqdm(range(to_load), total=to_load):\n",
    "    num_str = str(i)\n",
    "    while len(num_str) < 5:\n",
    "        num_str = '0' + num_str\n",
    "    try:\n",
    "        temp = pd.read_csv(f'/notebooks/comma/controls_challenge/data/{num_str}.csv', sep=',')\n",
    "    except:\n",
    "        print(f'data/{num_str}.csv does not exist')\n",
    "    dfs.append(temp)\n",
    "df = pd.concat(dfs)\n",
    "df['shifted_target'] = df['targetLateralAcceleration'].shift(1).fillna(0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import random\n",
    "\n",
    "\n",
    "class DriverEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"console\"]}\n",
    "\n",
    "    def __init__(self, driving_model, filenames=None, baselines=None):\n",
    "        super().__init__()\n",
    "        self.action_space = spaces.Box(\n",
    "            low = -1, high = 1, shape = (1,), dtype = np.float32\n",
    "        )\n",
    "        self.driving_model = driving_model\n",
    "        self.physics_simulator = None\n",
    "        self.data = None\n",
    "        self.file_n = 0\n",
    "        self.baseline_simulator = None\n",
    "        self.filenames = filenames\n",
    "        self.baselines = baselines if baselines is not None else {}\n",
    "        self.exploration_done = True\n",
    "        self.observation_space = spaces.Dict(\n",
    "            spaces={\n",
    "                'CURRENT_INFO': spaces.Box(low=-np.inf, high=np.inf, shape=(11,)),\n",
    "                'CURRENT_LATACCEL_DIFF': spaces.Box(low=-np.inf, high=np.inf, shape=(49,)),\n",
    "                'FUTURE_LATACCEL': spaces.Box(low=-np.inf, high=np.inf, shape=(49,)),\n",
    "                'FUTURE_ROLL': spaces.Box(low=-np.inf, high=np.inf, shape=(49,)),\n",
    "                'FUTURE_VEL': spaces.Box(low=-np.inf, high=np.inf, shape=(49,)),\n",
    "                'FUTURE_A': spaces.Box(low=-np.inf, high=np.inf, shape=(49,)),\n",
    "                'PAST_LATACCEL': spaces.Box(low=-np.inf, high=np.inf, shape=(20,)),\n",
    "                'PAST_ACTIONS': spaces.Box(low=-np.inf, high=np.inf, shape=(20,)),\n",
    "                'PAST_ROLL': spaces.Box(low=-np.inf, high=np.inf, shape=(20,)),\n",
    "                'PAST_VEL': spaces.Box(low=-np.inf, high=np.inf, shape=(20,)),\n",
    "                'PAST_A': spaces.Box(low=-np.inf, high=np.inf, shape=(20,))\n",
    "            })\n",
    "        \n",
    "    def calculate_cost(self, target, pred, prev_current_lataccel):\n",
    "        lat_accel_cost = (target - pred)**2 * 100\n",
    "        jerk_cost = ((pred - prev_current_lataccel) / DEL_T)**2 * 100\n",
    "            \n",
    "        total_cost = (lat_accel_cost * LAT_ACCEL_COST_MULTIPLIER) + jerk_cost\n",
    "        return total_cost\n",
    "    \n",
    "    def update_array(self, array, newVal, limit=20):\n",
    "        array = np.append(array, newVal)\n",
    "        if len(array) > limit:\n",
    "            array = np.delete(array, 0)\n",
    "        return array\n",
    "    \n",
    "    def step(self, action):\n",
    "        prev_current_lataccel = self.physics_simulator.current_lataccel_history[self.physics_simulator.step_idx - 1]\n",
    "        baseline_prev_current_lataccel = self.baseline_simulator.current_lataccel_history[self.physics_simulator.step_idx - 1]\n",
    "        prev_target_lataccel = self.physics_simulator.get_state_target_futureplan(self.physics_simulator.step_idx)[1]\n",
    "        act = action[0]*2\n",
    "        last_action = self.last_actions[-1] if len(self.last_actions) > 0 else act\n",
    "\n",
    "        self.physics_simulator.step(act)\n",
    "        current_lataccel = self.physics_simulator.current_lataccel_history[self.physics_simulator.step_idx - 1]\n",
    "        baseline_current_lataccel = self.baseline_simulator.current_lataccel_history[self.physics_simulator.step_idx - 1]\n",
    "        \n",
    "        terminated = self.physics_simulator.step_idx == COST_END_IDX - 1\n",
    "        if terminated:\n",
    "            print(f'cost for {self.filename} is {self.physics_simulator.compute_cost()}')\n",
    "            s = self.physics_simulator.get_state_target_futureplan(self.physics_simulator.step_idx - 1)\n",
    "        else:\n",
    "            s = self.physics_simulator.get_state_target_futureplan(self.physics_simulator.step_idx)\n",
    "            \n",
    "        state = s[0]\n",
    "        target_lataccel = s[1]\n",
    "        future = FuturePlan(\n",
    "            lataccel=s[2].lataccel if len(s[2].lataccel) > 0 else [current_lataccel],\n",
    "            roll_lataccel=s[2].roll_lataccel if len(s[2].roll_lataccel) > 0 else [state.roll_lataccel],\n",
    "            v_ego=s[2].v_ego if len(s[2].v_ego) > 0 else [state.v_ego],\n",
    "            a_ego=s[2].a_ego if len(s[2].a_ego) > 0 else [state.a_ego]\n",
    "        )\n",
    "        \n",
    "        self.last_actions = self.update_array(self.last_actions, act, 10)\n",
    "        error = target_lataccel - current_lataccel\n",
    "        self.error_integral += error\n",
    "        error_diff = error - self.prev_error\n",
    "        self.prev_error = error\n",
    "        pid_action = error*0.085 + self.error_integral*0.11\n",
    "        \n",
    "        action_diff = act - last_action\n",
    "            \n",
    "        delta_cost = 0\n",
    "        actual_cost = self.calculate_cost(self.physics_simulator.target_lataccel_history[-1], current_lataccel, prev_current_lataccel)\n",
    "        baseline_cost = self.calculate_cost(self.physics_simulator.target_lataccel_history[-1], baseline_current_lataccel, baseline_prev_current_lataccel)\n",
    "        reward = baseline_cost - actual_cost\n",
    "        if abs(action_diff) > 0.3:\n",
    "            delta_cost = abs(action_diff)\n",
    "            reward -= delta_cost*1000\n",
    "            \n",
    "        self.rewards = np.append(self.rewards, reward)\n",
    "        future_diffs = np.diff(np.array(future.lataccel))\n",
    "        observation = {\n",
    "            'CURRENT_INFO': np.array([\n",
    "                target_lataccel, current_lataccel, action_diff, last_action,\n",
    "                error, self.prev_error, np.clip(self.error_integral, 100, -100),\n",
    "                np.mean((np.diff(np.array(future.lataccel[:10])) / DEL_T)**2), np.mean((future_diffs / DEL_T)**2),\n",
    "                np.mean(future.lataccel[:10]), np.mean(future.lataccel),\n",
    "            ]), \n",
    "            'CURRENT_LATACCEL_DIFF': np.array(future.lataccel - current_lataccel), \n",
    "            'FUTURE_LATACCEL': np.array(future.lataccel), \n",
    "            'FUTURE_ROLL':np.array(future.roll_lataccel), \n",
    "            'FUTURE_VEL':np.array(future.v_ego), \n",
    "            'FUTURE_A':np.array(future.a_ego),\n",
    "            'PAST_LATACCEL':np.array(self.physics_simulator.current_lataccel_history[-19:] + [current_lataccel]),\n",
    "            'PAST_ACTIONS': np.array(self.physics_simulator.action_history[-20:]),\n",
    "            'PAST_ROLL': np.array([s.roll_lataccel for s in self.physics_simulator.state_history[-19:]] + [state.roll_lataccel]),\n",
    "            'PAST_VEL': np.array([s.v_ego for s in self.physics_simulator.state_history[-19:]] + [state.v_ego]),\n",
    "            'PAST_A': np.array([s.a_ego for s in self.physics_simulator.state_history[-19:]] + [state.a_ego]),\n",
    "        }\n",
    "\n",
    "        if terminated:\n",
    "            #print(pd.DataFrame(self.observations).describe())\n",
    "            print(reward, np.mean(self.rewards))\n",
    "            return observation, reward, terminated, False, {}\n",
    "\n",
    "        truncated = False\n",
    "        info = {}\n",
    "            \n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.filename = None\n",
    "        self.filename = self.filenames[self.file_n]\n",
    "        self.file_n += 1\n",
    "        #n_file = random.randint(0, len(self.filenames) - 1)\n",
    "        #self.filename = self.filenames[n_file]\n",
    "            \n",
    "        if self.filenames is None:\n",
    "            #n_file = random.randint(0, 20000)\n",
    "            n_file = self.file_n\n",
    "            num_str = str(n_file)\n",
    "            while len(num_str) < 5:\n",
    "                num_str = '0' + num_str\n",
    "            self.filename = f'/notebooks/comma/controls_challenge/data/{num_str}.csv'\n",
    "\n",
    "        if self.file_n >= 20_000 or (self.filenames is not None and self.file_n >= len(self.filenames)):\n",
    "            self.file_n = 0\n",
    "        self.data = pd.read_csv(self.filename, sep=',')\n",
    "        self.cum_error = 0\n",
    "        self.prev_error = 0\n",
    "        self.observations = []\n",
    "        self.last_actions = np.array([])\n",
    "        self.rewards = np.array([])\n",
    "        self.last_action = 0\n",
    "        self.error_integral = 0\n",
    "        self.prev_error = 0\n",
    "        controller2 = pid.Controller()\n",
    "        controller = pid.Controller()\n",
    "        self.physics_simulator = MySimulator(self.driving_model, self.filename, controller=controller2, debug=False)\n",
    "        if self.filename in self.baselines: \n",
    "            self.baseline_simulator = self.baselines[self.filename]\n",
    "            cost = self.baseline_simulator.compute_cost()\n",
    "            print(f\"Retrieving cost for {self.filename}: {cost}\")\n",
    "        else:\n",
    "            self.baseline_simulator = TinyPhysicsSimulator(self.driving_model, self.filename, controller=controller, debug=False)\n",
    "            cost = self.baseline_simulator.rollout()\n",
    "            print(f\"Calculating cost for {self.filename}: {cost}\")\n",
    "            self.baselines[self.filename] = self.baseline_simulator\n",
    "        for i in range(20, 99):\n",
    "            self.physics_simulator.step(self.data.iloc[i]['steerCommand'])\n",
    "        observation, _, _, _, info = self.step(np.array([self.data.iloc[99]['steerCommand']*2]).astype(np.float32))\n",
    "        return observation, info\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_value(lower_bound, upper_bound, value):\n",
    "    og_min = -1\n",
    "    og_max = 1\n",
    "\n",
    "    # Apply the linear transformation formula\n",
    "    transformed_value = lower_bound + (value - og_min) * (upper_bound - lower_bound) / (og_max - og_min)\n",
    "\n",
    "    return transformed_value\n",
    "\n",
    "def inverse_transform_value(lower_bound, upper_bound, transformed_value):\n",
    "    # Apply the inverse linear transformation formula\n",
    "    original_value = (2 * (transformed_value - lower_bound) / (upper_bound - lower_bound)) - 1\n",
    "\n",
    "    return original_value\n",
    "\n",
    "print(transform_value(0, 0.6, -0.7165))\n",
    "print(inverse_transform_value(0, 0.6, 0.085))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch as th\n",
    "from stable_baselines3.common.preprocessing import get_flattened_obs_dim\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class CustomCombinedExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Dict):\n",
    "        super().__init__(observation_space, features_dim=1)\n",
    "\n",
    "        extractors: Dict[str, nn.Module] = {}\n",
    "\n",
    "        total_concat_size = 0\n",
    "        for key, subspace in observation_space.spaces.items():\n",
    "            if key != \"CURRENT_INFO\":\n",
    "                # Since the arrays have some local correlation we can reduce the dimensionality using first an average pooling and then passing it through an Linear layer.\n",
    "                extractors[key] = nn.Sequential(nn.AvgPool1d(5), nn.Linear(subspace.shape[0] // 5, 5), nn.Flatten())\n",
    "                total_concat_size += 5\n",
    "            else:\n",
    "                # The current info vector has no relationship between elements, so we just flatten it.\n",
    "                extractors[key] = nn.Flatten()\n",
    "                total_concat_size += get_flattened_obs_dim(subspace)\n",
    "\n",
    "        self.extractors = nn.ModuleDict(extractors)\n",
    "\n",
    "        # Update the features dim manually\n",
    "        self._features_dim = total_concat_size\n",
    "\n",
    "    def forward(self, observations) -> th.Tensor:\n",
    "        encoded_tensor_list = []\n",
    "\n",
    "        for key, extractor in self.extractors.items():\n",
    "            encoded_tensor_list.append(extractor(observations[key]))\n",
    "        return th.cat(encoded_tensor_list, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the relevant filenames according to the selectFiles.py script\n",
    "with open('top500_20k.pkl', 'rb') as handle:\n",
    "    filenames = pickle.load(handle)\n",
    "filenames = sorted(list(set(filenames + ['data/00000.csv', 'data/00001.csv', 'data/00002.csv', 'data/00003.csv', 'data/00004.csv', 'data/00005.csv'])))\n",
    "#sorted(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "def make_env(env_id, rank, seed=0):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: (str) the environment ID\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    \"\"\"\n",
    "\n",
    "    def _init():\n",
    "        env = DriverEnv(driving_model, None, False, 0, filenames)\n",
    "        # use a seed for reproducibility\n",
    "        # Important: use a different seed for each environment\n",
    "        # otherwise they would generate the same experiences\n",
    "        env.reset(seed=seed + rank)\n",
    "        return env\n",
    "\n",
    "    set_random_seed(seed)\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "config = {\n",
    "    \"policy_type\": \"MultiInputPolicy\",\n",
    "    \"total_timesteps\": 1_000_000,\n",
    "    \"env_name\": \"DriverEnv\",\n",
    "}\n",
    "run = wandb.init(\n",
    "    project=\"sb3-comma\",\n",
    "    config=config,\n",
    "    sync_tensorboard=True,\n",
    "    monitor_gym=False,\n",
    "    save_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script that generates baselines for a subset of filenames. Useful when debugging, to avoid recalculating again and again.\n",
    "def generate_baselines(filenames, driving_model):\n",
    "    baselines = {}\n",
    "    for file in tqdm(filenames, total=len(filenames)):\n",
    "        baseline_controller = pid.Controller()\n",
    "        baseline_simulator = TinyPhysicsSimulator(driving_model, file, controller=baseline_controller, debug=False) #TinyPhysicsSimulator\n",
    "        cost = baseline_simulator.rollout()\n",
    "        # print(f\"Calculating cost for {file}: {cost}\")\n",
    "        baselines[file] = baseline_simulator\n",
    "    return baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "baselines = generate_baselines(filenames, driving_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, A2C, TD3, SAC\n",
    "from sb3_contrib import TRPO, RecurrentPPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize, SubprocVecEnv\n",
    "from sb3_contrib.common.recurrent.policies import RecurrentActorCriticPolicy\n",
    "\n",
    "n_procs = 8 # adjust to your hardware.\n",
    "driving_model = MyPhysicsModel(\"./models/tinyphysics.onnx\", debug=False) # Load a driving model with low temperature\n",
    "checkpoint_callback = CheckpointCallback(save_freq=5e4, save_path='tmp/model_checkpoints2/', save_vecnormalize=True) # Save model each 50k steps for large training runs.\n",
    "env = DriverEnv(driving_model, filenames[:1], None)\n",
    "env = make_vec_env(lambda: env, n_envs=1) # Use this when debugging.\n",
    "#env = SubprocVecEnv([make_env(0, i, i*10) for i in range(n_procs)], start_method=\"fork\") # Use this when training.\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=300, clip_reward=np.inf) # ALWAYS normalize.\n",
    "policy_kwargs = dict(\n",
    "    normalize_images=False, # We do not need to normalize image, since we are not using images to train.\n",
    "    features_extractor_class=CustomCombinedExtractor, # We set the extractor defined previously\n",
    ")\n",
    "model = TRPO(\"MultiInputPolicy\", env, verbose=1, batch_size=256, learning_rate=linear_schedule(0.001), policy_kwargs=policy_kwargs)\n",
    "#model.set_logger(new_logger) # In case we want to set a different logger. Useful for large training runs.\n",
    "model.learn(total_timesteps=15_000_000, log_interval = 1, callback=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close() # Do this after a SubprocVecEnv run or you will run into memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=15_000_000, log_interval = 1, callback=[\n",
    "    WandbCallback(\n",
    "        gradient_save_freq=100,\n",
    "        model_save_path=f\"models/{run.id}\",\n",
    "        verbose=2,\n",
    "    ), checkpoint_callback]) # In case we need to save the logs to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to save the VecNormalize statistics when saving the agent\n",
    "import os\n",
    "log_dir = \"/notebooks/comma/tmp/\"\n",
    "model.save(log_dir + \"TRPO_Steering\")\n",
    "stats_path = os.path.join(log_dir, \"TRPO_Steering_vec_normalize.pkl\")\n",
    "env.save(stats_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3 import PPO, A2C, TD3, SAC\n",
    "from sb3_contrib import TRPO, RecurrentPPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize, VecFrameStack\n",
    "\n",
    "# Commands to load the model.\n",
    "env = DriverEnv(driving_model, filenames, baselines)\n",
    "log_dir = \"/notebooks/comma/tmp/\"\n",
    "stats_path = os.path.join(log_dir, \"TRPO_Steering_vec_normalize.pkl\")\n",
    "env = make_vec_env(lambda: env, n_envs=1)\n",
    "env = VecNormalize.load(stats_path, env)\n",
    "# Set the training and norm flags accordingly if you want to train more or not.\n",
    "env.training = True\n",
    "env.norm_reward = True\n",
    "env.norm_obs = True\n",
    "env.clip_reward=np.inf\n",
    "env.clip_obs=300\n",
    "\n",
    "# Load the agent\n",
    "model = TRPO.load(log_dir + \"TRPO_Steering\", env=env, learning_rate=linear_schedule(0.001), batch_size=256, seed=42, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "tmp_path = \"/notebooks/comma/tmp/logs/\"\n",
    "# set up logger\n",
    "new_logger = configure(tmp_path, [\"stdout\", \"csv\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
